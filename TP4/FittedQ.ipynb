{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6378dcd6-9bb7-48d1-a095-7727a24b7173",
      "metadata": {
        "id": "6378dcd6-9bb7-48d1-a095-7727a24b7173"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "- Follow the installation instructions in the readme file\n",
        "- Answer the questions in this notebook\n",
        "- Once your work is finished: restart the kernel, run all cells in order and check that the outputs are correct.\n",
        "- Send your completed notebook to `remy.degenne@inria.fr` with email title `SL_TP4_NAME1_NAME2` (or `SL_TP4_NAME` if you work alone).\n",
        "\n",
        "**Deadline: February 13, 11:59 AM CET**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6354267",
      "metadata": {
        "id": "d6354267"
      },
      "source": [
        "If you don't want to use a local installation, you can try Google Colab:\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/RemyDegenne/remydegenne.github.io/blob/master/docs/SL_2025/FittedQ.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ea0e1ea",
      "metadata": {
        "id": "2ea0e1ea"
      },
      "outputs": [],
      "source": [
        "# This cell is setting up google colab. Ignore it if you work locally.\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  print(\"Installing packages, please wait a few moments. Restart the runtime after the installation.\")\n",
        "  # install rlberry library\n",
        "  !pip install scipy scikit_learn git+https://github.com/rlberry-py/rlberry@v0.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aef0b14a-6e93-4f0b-be94-84d090c5d184",
      "metadata": {
        "id": "aef0b14a-6e93-4f0b-be94-84d090c5d184"
      },
      "source": [
        "# Fitted Q Iteration (FQI)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will implement the Fitted Q Iteration algorithm (FQI) to solve the [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) problem.\n",
        "\n",
        "This notebooks will first cover the basics for using the Gymnasium library: how to instantiate an environment, step into it and collect training data from the FQI algorithm.\n",
        "\n",
        "You will then learn how to implement step-by-step the FQI algorithm which is the predecessor of the [Deep Q-Network (DQN)](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "008852aa-cf8f-4c81-afaf-7d3be75f389d",
      "metadata": {
        "id": "008852aa-cf8f-4c81-afaf-7d3be75f389d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import os\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.base import RegressorMixin\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.neighbors import KNeighborsRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2369b55-266c-4dbe-b14d-250ef7386407",
      "metadata": {
        "id": "e2369b55-266c-4dbe-b14d-250ef7386407"
      },
      "source": [
        "## First steps with the Gym interface\n",
        "\n",
        "An environment that follows the [gym interface](https://gymnasium.farama.org/) is quite simple to use.\n",
        "It provides to this user mainly three methods, which have the following signature (for gym versions > 0.26):\n",
        "\n",
        "- `reset()` called at the beginning of an episode, it returns an observation and a dictionary with additional info (defaults to an empty dict)\n",
        "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether new state is a terminal state (episode is finished), whether the max number of timesteps is reached (episode is artificially finished), and additional information\n",
        "- (Optional) `render()` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `render_mode='rbg_array'` to retrieve an image of the scene).\n",
        "\n",
        "Under the hood, it also contains two useful properties:\n",
        "- `observation_space` which is one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
        "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
        "\n",
        "The best way to learn about [gym spaces](https://gymnasium.farama.org/api/spaces/) is to look at the [source code](https://github.com/Farama-Foundation/Gymnasium/tree/main/gymnasium/spaces), but you need to know at least the main ones:\n",
        "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of $[a, b]$, $(-\\infty, b]$, $[a, \\infty)$, or $(-\\infty, \\infty)$. Example: A 1D-Vector or an image observation can be described with the Box space.\n",
        "```python\n",
        "# Example for using image as input:\n",
        "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
        "```                                       \n",
        "\n",
        "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
        "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af049724-3db9-4dec-a40c-3aa025735f00",
      "metadata": {
        "id": "af049724-3db9-4dec-a40c-3aa025735f00"
      },
      "source": [
        "## CartPole Environment\n",
        "\n",
        "For this example, we will use CartPole environment, a classic control problem.\n",
        "\n",
        "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
        "\n",
        "Cartpole environment: [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
        "\n",
        "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "60ec422f-2edd-40df-8da2-c1093e1da38c",
      "metadata": {
        "id": "60ec422f-2edd-40df-8da2-c1093e1da38c"
      },
      "outputs": [],
      "source": [
        "# Instantiate the environment\n",
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "476f88b6-4fe2-45ec-bddd-4d83feb3a86f",
      "metadata": {
        "id": "476f88b6-4fe2-45ec-bddd-4d83feb3a86f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Shape: (4,)\n",
            "Action space: Discrete(2)\n"
          ]
        }
      ],
      "source": [
        "# Box(4,) means that it is a Vector with 4 components\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Shape:\", env.observation_space.shape)\n",
        "\n",
        "# Discrete(2) means that there is two discrete actions\n",
        "print(\"Action space:\", env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e414990e-2cbd-4f6f-b268-42f16a9b253b",
      "metadata": {
        "id": "e414990e-2cbd-4f6f-b268-42f16a9b253b"
      },
      "outputs": [],
      "source": [
        "# The reset method is called at the beginning of an episode\n",
        "obs, info = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e89a95c8-d64a-43a3-adee-344891bbd1b7",
      "metadata": {
        "id": "e89a95c8-d64a-43a3-adee-344891bbd1b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled action: 1\n"
          ]
        }
      ],
      "source": [
        "# Sample a random action\n",
        "action = env.action_space.sample()\n",
        "print(f\"Sampled action: {action}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "311777a2-9518-496a-b1e8-0eb1af81fb7d",
      "metadata": {
        "id": "311777a2-9518-496a-b1e8-0eb1af81fb7d"
      },
      "outputs": [],
      "source": [
        "# step in the environment\n",
        "obs, reward, terminated, truncated, info = env.step(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "5102a9c8-8407-4f88-b58d-4e3e5a00af3b",
      "metadata": {
        "id": "5102a9c8-8407-4f88-b58d-4e3e5a00af3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4,) 1.0 False False {}\n"
          ]
        }
      ],
      "source": [
        "# Note the obs is a numpy array\n",
        "# info is an empty dict for now but can contain any debugging info\n",
        "# reward is a scalar\n",
        "print(obs.shape, reward, terminated, truncated, info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90fbec50-1da0-4bc4-8b88-79fa38480bfe",
      "metadata": {
        "id": "90fbec50-1da0-4bc4-8b88-79fa38480bfe"
      },
      "source": [
        "### Exercise: write the function to collect data\n",
        "\n",
        "This function collects a dataset of transitions that will be used to train a model using the FQI algorithm.\n",
        "\n",
        "See docstring of the function for what is expected as input/output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "1f1b3ff1-911b-4582-91f7-49420380eda3",
      "metadata": {
        "id": "1f1b3ff1-911b-4582-91f7-49420380eda3"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class OfflineData:\n",
        "    \"\"\"\n",
        "    A class to store transitions.\n",
        "    \"\"\"\n",
        "\n",
        "    observations: np.ndarray  # same as \"state\" in the theory\n",
        "    next_observations: np.ndarray\n",
        "    actions: np.ndarray\n",
        "    rewards: np.ndarray\n",
        "    terminateds: np.ndarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f9f753ed-5f5d-4133-ab82-c84b3cfe2eef",
      "metadata": {
        "id": "f9f753ed-5f5d-4133-ab82-c84b3cfe2eef"
      },
      "outputs": [],
      "source": [
        "def collect_data(env: gym.Env, n_steps: int = 50_000) -> OfflineData:\n",
        "    \"\"\"\n",
        "    Collect transitions using a random agent (sample action randomly).\n",
        "\n",
        "    :param env: The environment.\n",
        "    :param n_steps: Number of steps to perform in the env.\n",
        "    :return: The collected transitions.\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(env.observation_space, spaces.Box)\n",
        "    # Numpy arrays (buffers) to collect the data\n",
        "    observations = np.zeros((n_steps, *env.observation_space.shape))\n",
        "    next_observations = np.zeros((n_steps, *env.observation_space.shape))\n",
        "    # Discrete actions\n",
        "    actions = np.zeros((n_steps, 1))\n",
        "    rewards = np.zeros((n_steps,))\n",
        "    terminateds = np.zeros((n_steps,))\n",
        "\n",
        "    # Variable to know if the episode is over (done = terminated or truncated)\n",
        "    done = False\n",
        "    # Start the first episode\n",
        "    obs, _ = env.reset()\n",
        "\n",
        "    for t in range(n_steps):\n",
        "        # 1. Sample a random action\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        # 2. Step in the env using this random action\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # 3. Retrieve the new transition data and update the numpy arrays\n",
        "        observations[t] = obs\n",
        "        next_observations[t] = next_obs\n",
        "        actions[t] = action\n",
        "        rewards[t] = reward\n",
        "        terminateds[t] = terminated\n",
        "\n",
        "        # Update the current observation\n",
        "        obs = next_obs\n",
        "\n",
        "        # Handle episode ending\n",
        "        if terminated or truncated:\n",
        "            obs, _ = env.reset()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "\n",
        "    return OfflineData(\n",
        "        observations,\n",
        "        next_observations,\n",
        "        actions,\n",
        "        rewards,\n",
        "        terminateds,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe5d4b3-d3d6-4f82-9337-a8566c1fc707",
      "metadata": {
        "id": "dbe5d4b3-d3d6-4f82-9337-a8566c1fc707"
      },
      "source": [
        "Let's try the collect data method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f499a940-a35f-4b8c-86bb-94231c41a87e",
      "metadata": {
        "id": "f499a940-a35f-4b8c-86bb-94231c41a87e"
      },
      "outputs": [],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "env = gym.make(env_id)\n",
        "n_steps = 10_000\n",
        "# Collect transitions for n_steps\n",
        "data = collect_data(env=env, n_steps=n_steps)\n",
        "\n",
        "# Check the length of the collected data\n",
        "assert len(data.observations) == n_steps\n",
        "assert len(data.actions) == n_steps\n",
        "# Check that there are multiple episodes in the data\n",
        "assert not np.all(data.terminateds)\n",
        "assert np.any(data.terminateds)\n",
        "# Check the shape of the collected data\n",
        "if env_id == \"CartPole-v1\":\n",
        "    assert data.observations.shape == (n_steps, 4)\n",
        "    assert data.next_observations.shape == (n_steps, 4)\n",
        "assert data.actions.shape == (n_steps, 1)\n",
        "assert data.rewards.shape == (n_steps,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "239ad0de-bec9-4918-a691-4322d062c45a",
      "metadata": {
        "id": "239ad0de-bec9-4918-a691-4322d062c45a"
      },
      "source": [
        "## Fitted Q Iteration (FQI) Agent\n",
        "\n",
        "See Lecture 4, slide 31 (and next slides for more explanations in the linear case, although this practical session is not linear).\n",
        "\n",
        "At each iteration of the algorithm, a dataset of transitions is gathered. Then target Q values for each transition are computed and the algorithm solves a regression problem with the transitions as inputs and the target values as outputs to update its Q-value approximation.\n",
        "\n",
        "After the maximal number of iterations is reached, the policy returned is the greedy policy with respect to the current Q-values.\n",
        "\n",
        "### Choosing a regression model\n",
        "\n",
        "With FQI, you can use any regression model to produce a Q-value estimator from a dataset of transitions and targets.\n",
        "\n",
        "Here we are choosing a [k-nearest neighbors regressor](https://scikit-learn.org/stable/modules/neighbors.html#regression), but one could choose a linear model, a decision tree, a neural network, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "1d8d873e-bc40-4a35-b98b-4fe9ce916aab",
      "metadata": {
        "id": "1d8d873e-bc40-4a35-b98b-4fe9ce916aab"
      },
      "outputs": [],
      "source": [
        "# First choose the regressor\n",
        "model_class = partial(\n",
        "    KNeighborsRegressor, n_neighbors=30\n",
        ")  # LinearRegression, GradientBoostingRegressor..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf9544c-87f4-4872-9ae9-eef49c3a040a",
      "metadata": {
        "id": "5bf9544c-87f4-4872-9ae9-eef49c3a040a"
      },
      "source": [
        "### 1. Exercise: write the function to predict Q-Values\n",
        "\n",
        "In FQI, we will need to compute, for any transition $(s, a, r, s')$, a target value $y = r + \\gamma \\cdot \\max_{a' \\in A}(Q^{n-1}_\\theta(s', a'))$. In order to do that, we need to be able to compute the current Q-value of state-action pairs.\n",
        "\n",
        "See docstring of the function for what is expected as input/output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "38316a3a",
      "metadata": {
        "id": "38316a3a"
      },
      "outputs": [],
      "source": [
        "def get_q_values(\n",
        "    model: RegressorMixin,\n",
        "    obs: np.ndarray,\n",
        "    n_actions: int,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Retrieve the q-values for a set of observations obs.\n",
        "    Q(s, action) for all s in obs and all possible actions.\n",
        "\n",
        "    :param model: Q-value estimator\n",
        "    :param obs: A batch of observations\n",
        "    :param n_actions: Number of discrete actions.\n",
        "    :return: The predicted q-values for the given observations\n",
        "        (batch_size, n_actions)\n",
        "    \"\"\"\n",
        "    batch_size = len(obs)\n",
        "    q_values = np.zeros((batch_size, n_actions))\n",
        "\n",
        "    for a in range(n_actions):\n",
        "        # 1. Create the regression model input (s, a) for the action a\n",
        "        # Create a column vector of size (batch_size, 1) filled with the current action 'a'\n",
        "        actions_col = np.full((batch_size, 1), a)\n",
        "        \n",
        "        # Concatenate observations and this specific action\n",
        "        # Input shape becomes (batch_size, state_dim + 1)\n",
        "        model_input = np.concatenate((obs, actions_col), axis=1)\n",
        "        \n",
        "        # 2. Predict the q-values for the batch of states (use model.predict)\n",
        "        q_pred = model.predict(model_input)\n",
        "        \n",
        "        # 3. Update q-values array for the current action a\n",
        "        q_values[:, a] = q_pred\n",
        "        \n",
        "    return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be7a24b4-a416-4607-af3a-9dfdbc203fc1",
      "metadata": {
        "id": "be7a24b4-a416-4607-af3a-9dfdbc203fc1"
      },
      "source": [
        "### Create the Agent\n",
        "To create an agent, rlberry requires to use a **very simple interface**, with basically two methods to implement: `fit()` and `eval()`.\n",
        "\n",
        "You can find more information on this interface [here(AgentWithSimplePolicy)](rlberry.agents.agent.AgentWithSimplePolicy).\n",
        "### function fit() :\n",
        "\n",
        "#### 1. First Iteration\n",
        "\n",
        "For $n = 0$, the initial training set is defined as:\n",
        "\n",
        "- $x = (s_t, a_t)$\n",
        "- $y = r_t$\n",
        "\n",
        "We fit a regression model $f_\\theta(x) = y$ to obtain $ Q^{n=0}_\\theta(s, a) $\n",
        "\n",
        "\n",
        "\n",
        "#### 2. Exercise: the fitted Q iterations\n",
        "\n",
        "1. Create the training set based on the previous iteration $ Q^{n-1}_\\theta(s, a) $ and the transitions:\n",
        "- input: $x = (s_t, a_t)$\n",
        "- if $s_{t+1}$ is non-terminal: $y = r_t + \\gamma \\cdot \\max_{a' \\in A}(Q^{n-1}_\\theta(s_{t+1}, a'))$\n",
        "- if $s_{t+1}$ is terminal, do not bootstrap: $y = r_t$\n",
        "\n",
        "2. Fit a model $f_\\theta$ using a regression algorithm to obtain $ Q^{n}_\\theta(s, a)$\n",
        "\n",
        "\\begin{aligned}\n",
        " f_\\theta(x) = y\n",
        "\\end{aligned}\n",
        "\n",
        "4. Repeat, $n = n + 1$\n",
        "\n",
        "### function evaluate() :\n",
        "\n",
        "#### 3. Exercise: write the function to evaluate a model\n",
        "\n",
        "A greedy policy $\\pi(s)$ can be defined using the q-value:\n",
        "\n",
        "$\\pi(s) = argmax_{a \\in A} Q(s, a)$.\n",
        "\n",
        "It is the policy that take the action with the highest q-value for a given state.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1fe29bd2-ea95-4778-b466-537a088615b0",
      "metadata": {
        "id": "1fe29bd2-ea95-4778-b466-537a088615b0"
      },
      "outputs": [],
      "source": [
        "import gymnasium.logger\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "# Patch : on définit une fausse fonction set_level pour éviter le crash de rlberry\n",
        "gymnasium.logger.set_level = lambda level: None\n",
        "\n",
        "from rlberry.agents import Agent\n",
        "\n",
        "# NOTE: On retire l'import de VideoRecorder car il n'existe plus dans cette version\n",
        "# from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder \n",
        "\n",
        "class Fitted_Q_Iteration(Agent):\n",
        "    name = \"Fitted_Q_Iteration\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        model_class,\n",
        "        n_steps_collection=50_000,\n",
        "        eval_freq=2,\n",
        "        n_eval_episodes=10,\n",
        "        gamma=0.99,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(env=env, **kwargs)\n",
        "\n",
        "        self.model_class = model_class\n",
        "        self.eval_freq = eval_freq\n",
        "        self.n_eval_episodes = n_eval_episodes\n",
        "        self.gamma = 0.99\n",
        "        self.n_actions = int(self.env.action_space.n)\n",
        "\n",
        "        # Collect data\n",
        "        self.current_data = collect_data(env=self.env, n_steps=n_steps_collection)\n",
        "\n",
        "        # 1 - First iteration\n",
        "        targets = self.current_data.rewards.copy()\n",
        "        self.current_obs_input = np.concatenate(\n",
        "            (self.current_data.observations, self.current_data.actions), axis=1\n",
        "        )\n",
        "        self.current_model = model_class().fit(self.current_obs_input, targets)\n",
        "\n",
        "    def fit(self, budget, **kwargs):\n",
        "        final_eval_result = 0.0\n",
        "        # 2 - the fitted Q iterations\n",
        "        for iter_idx in range(budget):\n",
        "            # 1. Compute next q values\n",
        "            next_q_values = get_q_values(\n",
        "                self.current_model,\n",
        "                self.current_data.next_observations,\n",
        "                self.n_actions\n",
        "            )\n",
        "\n",
        "            # 2. Greedy policy (max over actions)\n",
        "            next_values = np.max(next_q_values, axis=1)\n",
        "\n",
        "            # 3. Construct target (TD(0))\n",
        "            targets = self.current_data.rewards + \\\n",
        "                      self.gamma * next_values * (1 - self.current_data.terminateds)\n",
        "\n",
        "            # 4. Fit new model\n",
        "            self.current_model = self.model_class().fit(self.current_obs_input, targets)\n",
        "\n",
        "            if (iter_idx + 1) % self.eval_freq == 0:\n",
        "                print(f\"Iter {iter_idx + 1}\")\n",
        "                print(f\"Score: {self.current_model.score(self.current_obs_input, targets):.2f}\")\n",
        "                final_eval_result = self.eval(self.n_eval_episodes)\n",
        "\n",
        "        info = {\"final_eval_result\": final_eval_result}\n",
        "        return info\n",
        "\n",
        "    def eval(\n",
        "        self,\n",
        "        n_simulation: int = 10,\n",
        "        video_name: Optional[str] = None,\n",
        "    ) -> float:\n",
        "        episode_returns = []\n",
        "        total_episodes = 0\n",
        "        \n",
        "        if not hasattr(self, \"eval_env\") or not self.eval_env:\n",
        "            self.eval_env = self.env\n",
        "\n",
        "        # --- PARTIE VIDEO DÉSACTIVÉE POUR EVITER LE CRASH ---\n",
        "        # video_recorder = None\n",
        "        # if video_name is not None and self.eval_env.render_mode == \"rgb_array\":\n",
        "        #     os.makedirs(\"./logs/videos/\", exist_ok=True)\n",
        "        #     # L'ancien VideoRecorder n'est plus compatible\n",
        "        #     pass \n",
        "\n",
        "        obs, _ = self.eval_env.reset()\n",
        "        n_actions = int(self.eval_env.action_space.n)\n",
        "        \n",
        "        episode_reward = 0.0\n",
        "        \n",
        "        while total_episodes < n_simulation:\n",
        "            # if video_recorder is not None:\n",
        "            #     video_recorder.capture_frame()\n",
        "\n",
        "            # --- TON CODE ICI ---\n",
        "            # Retrieve q-values\n",
        "            q_vals = get_q_values(self.current_model, obs.reshape(1, -1), n_actions)\n",
        "            # Select greedy action\n",
        "            action = np.argmax(q_vals)\n",
        "            # Step\n",
        "            obs, reward, terminated, truncated, info = self.eval_env.step(action)\n",
        "            # --- FIN DE TON CODE ---\n",
        "\n",
        "            episode_reward += float(reward)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if done:\n",
        "                episode_returns.append(episode_reward)\n",
        "                episode_reward = 0.0\n",
        "                total_episodes += 1\n",
        "                obs, _ = self.eval_env.reset()\n",
        "\n",
        "        # if video_recorder is not None:\n",
        "        #     video_recorder.close()\n",
        "\n",
        "        print(\n",
        "            f\"Total reward = {np.mean(episode_returns):.2f} +/- {np.std(episode_returns):.2f}\"\n",
        "        )\n",
        "\n",
        "        return float(np.mean(episode_returns))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d824b98-6360-4d4e-a3e2-584c0c36665e",
      "metadata": {
        "id": "5d824b98-6360-4d4e-a3e2-584c0c36665e"
      },
      "source": [
        "### Performing experiments\n",
        "\n",
        "First, let's define some constants:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "c16a4e16-e2a8-432f-885f-27fbe0f61f7c",
      "metadata": {
        "id": "c16a4e16-e2a8-432f-885f-27fbe0f61f7c"
      },
      "outputs": [],
      "source": [
        "from rlberry.envs import gym_make\n",
        "from rlberry.manager import ExperimentManager\n",
        "\n",
        "# Max number of iterations\n",
        "n_iterations = 20\n",
        "# How often do we evaluate the learned model\n",
        "eval_freq = 2\n",
        "# How many episodes to evaluate every eval-freq\n",
        "n_eval_episodes = 10\n",
        "# discount factor\n",
        "gamma = 0.99\n",
        "# Number of discrete actions\n",
        "n_actions = int(env.action_space.n)\n",
        "\n",
        "\n",
        "env_id = \"CartPole-v1\"  # Id of the environment\n",
        "env_ctor = gym_make  # constructor for the env\n",
        "env_kwargs = dict(id=env_id)  # give the id of the env inside the kwargs\n",
        "\n",
        "eval_env_kwargs = dict(id=env_id, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeeaf39d-0a48-4c68-aa5b-7b734e7bcc7f",
      "metadata": {
        "id": "aeeaf39d-0a48-4c68-aa5b-7b734e7bcc7f"
      },
      "source": [
        "Now let's create an `ExperimentManager`, which is a class used to run experiments with specified agents and environments.\n",
        "\n",
        "The experiment manager spawns agents and environments for training and then once the agents are trained, it uses these agents and new environments to evaluate how well the agents perform. All of these steps can be done several times to assess stochasticity of agents and/or environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "7cbff0d8-eab0-4cec-847e-23bc1c509359",
      "metadata": {
        "id": "7cbff0d8-eab0-4cec-847e-23bc1c509359"
      },
      "outputs": [],
      "source": [
        "my_experiment = ExperimentManager(\n",
        "    Fitted_Q_Iteration,  # Agent Class\n",
        "    (env_ctor, env_kwargs),  # Environment as Tuple(constructor,kwargs)\n",
        "    init_kwargs=dict(\n",
        "        model_class=model_class,\n",
        "        n_steps_collection=50_000,\n",
        "        eval_freq=eval_freq,\n",
        "        n_eval_episodes=n_eval_episodes,\n",
        "        gamma=gamma,\n",
        "    ),\n",
        "    eval_env=(env_ctor, eval_env_kwargs),\n",
        "    fit_budget=int(n_iterations),  # Budget used to call our agent \"fit()\"\n",
        "    n_fit=1,  # Number of agent instances to fit.\n",
        "    agent_name=\"Agent_FQI_\" + env_id,  # Name of the agent\n",
        "    seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83f248b-14d3-468e-a218-d1164740006c",
      "metadata": {
        "id": "b83f248b-14d3-468e-a218-d1164740006c"
      },
      "source": [
        "Use `fit()` to train an agent and then `eval_agents()` to evaluate the trained agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "594a49dc-8187-4b13-9dcc-cd49148673ca",
      "metadata": {
        "id": "594a49dc-8187-4b13-9dcc-cd49148673ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running ExperimentManager fit() for Agent_FQI_CartPole-v1 with n_fit = 1 and max_workers = None."
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 2\n",
            "Score: 0.73\n",
            "Total reward = 22.20 +/- 9.89\n",
            "Iter 4\n",
            "Score: 0.86\n",
            "Total reward = 135.90 +/- 9.45\n",
            "Iter 6\n",
            "Score: 0.90\n",
            "Total reward = 157.80 +/- 27.95\n",
            "Iter 8\n",
            "Score: 0.92\n",
            "Total reward = 177.70 +/- 30.62\n",
            "Iter 10\n",
            "Score: 0.93\n",
            "Total reward = 173.60 +/- 27.33\n",
            "Iter 12\n",
            "Score: 0.93\n",
            "Total reward = 190.70 +/- 48.67\n",
            "Iter 14\n",
            "Score: 0.94\n",
            "Total reward = 337.20 +/- 58.65\n",
            "Iter 16\n",
            "Score: 0.94\n",
            "Total reward = 419.00 +/- 85.17\n",
            "Iter 18\n",
            "Score: 0.94\n",
            "Total reward = 361.90 +/- 100.73\n",
            "Iter 20\n",
            "Score: 0.94\n",
            "Total reward = 400.10 +/- 88.26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[38;21m[INFO] 15:21: ... trained! \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "my_experiment.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3477740-bfe4-4e0b-8a5d-cd0206786c8b",
      "metadata": {
        "id": "e3477740-bfe4-4e0b-8a5d-cd0206786c8b"
      },
      "source": [
        "### Record a video of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "da8d90cc-2d77-4682-a97c-a3dacd081f38",
      "metadata": {
        "id": "da8d90cc-2d77-4682-a97c-a3dacd081f38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO] Evaluation:Warning: n_simulation parameter in eval_kwargs is being overwritten with 1..Warning: n_simulation parameter in eval_kwargs is being overwritten with 1..  Evaluation finished \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward = 382.00 +/- 0.00\n",
            "Total reward = 500.00 +/- 0.00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[382.0, 500.0]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "video_name = f\"FQI_{env_id}\"\n",
        "n_eval_episodes = 3\n",
        "my_experiment.eval_agents(\n",
        "    eval_kwargs=dict(n_simulation=n_eval_episodes, video_name=video_name)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e84c6d08",
      "metadata": {
        "id": "e84c6d08"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "def show_videos(video_path: str = \"\", prefix: str = \"\") -> None:\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "    :param video_path: Path to the folder containing videos\n",
        "    :param prefix: Filter the video, showing only the only starting with this prefix\n",
        "    \"\"\"\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(f\"{prefix}*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "bfc66910-de41-4622-9811-96addf8ba8d3",
      "metadata": {
        "id": "bfc66910-de41-4622-9811-96addf8ba8d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FQI agent on CartPole-v1 after 20 iterations:\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(f\"FQI agent on {env_id} after {n_iterations} iterations:\")\n",
        "show_videos(\"./logs/videos/\", prefix=video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69a60230-6477-4318-8d60-10f6eada6064",
      "metadata": {
        "id": "69a60230-6477-4318-8d60-10f6eada6064"
      },
      "source": [
        "### Going further (optional)\n",
        "\n",
        "- play with different models, and with their hyperparameters\n",
        "- play with the discount factor\n",
        "- play with the number of data collected/used\n",
        "- combine data from random policy with data from trained model\n",
        "- Use a neural network as the regression model (Scikit-learn has a class for simple fully connected neural networks)\n",
        "- Implement DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b8a44f-d48c-4a9d-8ed4-968c1ffb9948",
      "metadata": {
        "id": "40b8a44f-d48c-4a9d-8ed4-968c1ffb9948"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "What we have seen in this notebook:\n",
        "- collecting data using a random agent in a gym environment\n",
        "- predicting q-values using a regression model\n",
        "- the fitted q-iteration (FQI) algorithm to learn from an offline dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "761010b8-86dd-4958-b097-2912606b4493",
      "metadata": {
        "id": "761010b8-86dd-4958-b097-2912606b4493"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cdc5f46",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fitted_q",
      "language": "python",
      "name": "fitted_q"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
