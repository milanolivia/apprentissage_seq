{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6cadc46-16e4-4152-a476-93d324cbb4d4",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- Follow the installation instructions in the readme file\n",
    "- Answer the questions in this notebook\n",
    "- Once your work is finished: restart the kernel, run all cells in order and check that the outputs are correct.\n",
    "- Send your completed notebook to `remy.degenne@inria.fr` with email title and notebook title `SL_TP2_NAME1_NAME2` (or `SL_TP2_NAME` if you work alone).\n",
    "\n",
    "**Deadline: Friday, January 9, 11:59 CET**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9bb56-0c9d-4f3e-b3e1-971f13589627",
   "metadata": {},
   "source": [
    "If you don't want to use a local installation, you can try Google Colab:\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/RemyDegenne/remydegenne.github.io/blob/master/docs/SL_2025/TD0_QLearning.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4bbd52-1a31-41d5-b9b6-a57db0d5d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is setting up google colab. Ignore it if you work locally.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print(\"Installing packages, please wait a few moments. Restart the runtime after the installation.\")\n",
    "  # install rlberry library\n",
    "  !pip install scipy rlberry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d22c13-8be3-49c5-9b0e-a483c8430fe4",
   "metadata": {},
   "source": [
    "# Small MDP with unknown dynamics: TD(0) and Q-Learing\n",
    "\n",
    "**The problem is the same as the last practical session: store management. The first few cells set up the environment and provide helpful tools like a policy iteration algorithm to compute the optimal policy if the MDP is known.**\n",
    "\n",
    "**The goal of this practical session is to implement algorithms for finding the optimal policy when the MDP is unknown.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c424d95-483a-49bf-9b46-518b9b4f3c7d",
   "metadata": {},
   "source": [
    "You own a bike store. During week $t$, the (random) demand is $D_t$ units. \n",
    "On Monday morning you may choose to command $A_t$ additional units: they are delivered immediately before the shop opens. For each week:\n",
    "\n",
    "  * Maintenance cost: $h$ per unit in stock left from the previous week (no maintenance is needed for newly commanded items)\n",
    "  * Command cost: $c$ for each unit ordered + $c_0$ per command\n",
    "  * Sales profit: $p$ per unit sold\n",
    "  * Constraint: \n",
    "    - your warehouse has a maximal capacity of $M$ unit (any additionnal bike gets stolen)\n",
    "    - you cannot sell bikes that you don't have in stock\n",
    "\n",
    "\n",
    "* State: number of bikes in stock left from the last week => state space $\\mathcal{S} = \\{0,\\dots, M\\}$\n",
    "* Action: number of bikes commanded at the beginning of the week => action space $\\mathcal{A} = \\{0, \\dots ,M\\}$ \n",
    "* Reward = balance of the week: if you command $a_t$ bikes,\n",
    "$$r_t = -c_0 \\mathbb{1}_{(a_t >0)}- c \\times a_t - h\\times s_t + p \\times \\min(D_t, s_t+a_t, M)$$\n",
    "* Transitions: you end the week with the number of bikes $$s_{t+1} = \\max\\big(0, \\min(M, s_t + a_t)  - D_t \\big)$$ \n",
    "\n",
    "Our goal is to maximize the discounted sum of rewards, starting from an initial stock $s_1$, that is to find a policy whose value is \n",
    "$$V^*(s_1) = \\max_{\\pi}\\mathbb{E}_{\\pi}\\left[\\sum_{s=1}^{\\infty} \\gamma^{s-1}r_s \\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d135b03-0584-4ac4-a136-ba322059933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38d7b6-99dc-47bc-bbd0-1480e6470a53",
   "metadata": {},
   "source": [
    "### Problem parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe96efd-6a07-418e-a46b-a51c3deac038",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 15 # stock capacity\n",
    "h = 0.3 # maintenance cost (per unit)\n",
    "c = 0.5 # ordering cost (per unit)\n",
    "c0 = 0.3 # fix delivery cost per command\n",
    "p = 1 # selling price (per unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666da9e8-156f-4b26-9b8e-b01477ae032e",
   "metadata": {},
   "source": [
    "### Specifying the demand distribution \n",
    "\n",
    "As an example of demand distribution, we choose a (truncated) geometric distribution, for which \n",
    "$$\\mathbb{P}(D_t = m) = q(1-q)^m \\ \\ \\forall m \\in \\{0,\\dots,M-1\\}$$\n",
    "and $\\mathbb{P}(D_t = M) = 1 - \\sum_{m=0}^{M-1}\\mathbb{P}(D_t = m)$. We provide below a function that simulate the demand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419982d4-5676-48af-951e-01180a395b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demand distribution (truncated geometric with parameter q)\n",
    "q = 0.1\n",
    "pdem = np.array([q*(1-q)**m for m in range(M+1)])\n",
    "pdem[M] = pdem[M]+1-np.sum(pdem)\n",
    "\n",
    "print(\"the average demand is \",np.sum([m*pdem[m] for m in range(M+1)]))\n",
    "\n",
    "def SimuDemand(pdem): \n",
    "    cpdem = np.cumsum(pdem)\n",
    "    i=0\n",
    "    u = rd.random()\n",
    "    while (u >cpdem[i]):\n",
    "        i = i+1\n",
    "    return i \n",
    "\n",
    "print(\"a simulated demand is \",SimuDemand(pdem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6e23b-28b4-4f4d-a302-e4137290624a",
   "metadata": {},
   "source": [
    "### Encoding the MDP as a gymnasium environment\n",
    "\n",
    "This is just a  toy example on how to create a gymnasium environement. \n",
    "\n",
    "Note that our environement is an example of tabular MDP for which the transition probabilities $P(s'|s,a)$ and the mean rewards $r(s,a)$ can actually be computed in closed form. Therefore we provide the transitions $(P)$ and mean rewards $(r)$ as attributes of the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69883b49-25a5-453e-ba46-414594c5c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextstate(s,a,d,M):\n",
    "    # computes the next state if the demand is d\n",
    "    return max(0,min(M,s+a) -d)\n",
    "\n",
    "def nextreward(s,a,d,M,c,c0,h,p):\n",
    "    # computes the reward if the demand is d\n",
    "    rew = -c*a - h*s + p*min(M,d,s+a)\n",
    "    if (a>0):\n",
    "        rew = rew - c0\n",
    "    return rew\n",
    "\n",
    "class StoreManagement(gym.Env):\n",
    "    \"\"\"\n",
    "    Retail Store Management environment\n",
    "    The environment defines which actions can be taken at which point and\n",
    "    when the agent receives which reward.\n",
    "    \"\"\"\n",
    "    def __init__(self,FirstState,M=15,h=0.3,c=0.5,c0=0.3,p=1,q=0.1):\n",
    "        \n",
    "        # General variables defining the environment\n",
    "        self.Stock_Capacity = M\n",
    "        self.Maintenance_Cost = h\n",
    "        self.Order_Cost = c \n",
    "        self.Delivery_Cost = c0\n",
    "        self.Selling_Price = p\n",
    "        pdem = np.array([q*(1-q)**m for m in range(M+1)])\n",
    "        pdem[M] = pdem[M]+1-np.sum(pdem)\n",
    "        self.Demand_Distribution = pdem\n",
    "        \n",
    "        # Define the action space\n",
    "        self.action_space = gym.spaces.Discrete(self.Stock_Capacity+1)\n",
    "\n",
    "        # Define the state space (state space = observation space in this example)\n",
    "        self.observation_space = gym.spaces.Discrete(self.Stock_Capacity+1)\n",
    "\n",
    "        # current time step\n",
    "        self.curr_step = -1 # \n",
    "        \n",
    "        # initial state\n",
    "        self.state = FirstState\n",
    "\n",
    "        # computation of the MDP parameters\n",
    "        P = np.zeros((M+1,M+1,M+1)) # P[s,a,s'] = p(s' | s,a) \n",
    "        r = np.zeros((M+1,M+1)) # r[s,a] = average reward received in state s when playing action a\n",
    "        ## iteration over all possible states, actions, and possible demand values\n",
    "        for a in range(M+1):\n",
    "            for s in range(M+1):\n",
    "                for d in range(M+1):\n",
    "                    # next state and reward with demand d\n",
    "                    ns = max(0,min(M,s+a) -d)\n",
    "                    reward = -c*a - h*s+p*min(M,d,s+a)\n",
    "                    if (a>0):\n",
    "                        reward = reward - c0\n",
    "                    P[s,a,ns] += pdem[d]\n",
    "                    r[s,a] += pdem[d] * reward\n",
    "        self.P = P # P[s,a,ns] = P(ns | s,a)\n",
    "        self.r = r # r[s,a] = r(s,a)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        simulates a transition following an action in the current state\n",
    "        action : int\n",
    "        \"\"\"\n",
    "        self.curr_step += 1\n",
    "        # simulate the demand \n",
    "        Demand = SimuDemand(self.Demand_Distribution)\n",
    "        # compute the reward\n",
    "        reward = nextreward(self.state,action,Demand,self.Stock_Capacity,self.Order_Cost,self.Delivery_Cost,self.Maintenance_Cost,self.Selling_Price)\n",
    "        # compute the next state \n",
    "        self.state = nextstate(self.state,action,Demand,self.Stock_Capacity)\n",
    "        # return 5 elements: observation / reward / termination? / truncation ? / information  \n",
    "        return self.state, reward, False, False,{}\n",
    "\n",
    "    def reset(self,InitialStock):\n",
    "        \"\"\"\n",
    "        Reset the state of the environment and returns an initial observation.\n",
    "        \"\"\"\n",
    "        self.curr_step = -1\n",
    "        self.state = InitialStock\n",
    "    \n",
    "    def _render(self, mode='human'):\n",
    "        \"\"\"optional visualization of the interaction: none here\"\"\"\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6917252-f398-473d-a4f0-9b6097570de2",
   "metadata": {},
   "source": [
    "### A function that simulates a trajectory under a policy Pi starting from some state $s_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e57801-1063-45c8-ba09-66a84a4edd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimulateTrajectory(T,Pi,s0):\n",
    "    \"\"\"return a vector of T successive states and a vector of T successive rewards\"\"\"\n",
    "    Rewards = np.zeros(T)\n",
    "    States = np.zeros(T)\n",
    "    env=StoreManagement(s0)\n",
    "    for t in range(T):\n",
    "        States[t]=env.state\n",
    "        action=Pi(env.state)\n",
    "        state,rew,_,_,_=env.step(action)\n",
    "        Rewards[t]=rew\n",
    "    return States,Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032a863a-9c4b-4952-a09e-6aff9b164e65",
   "metadata": {},
   "source": [
    "### Some simple policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab55575-5f66-44a2-9aab-05d1a551c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PiUniform(s):\n",
    "    # pick uniformly at random in {0,1,...,M-s}\n",
    "    x = rd.sample(range(M+1-s),1)\n",
    "    return s+x[0]\n",
    "\n",
    "def PiConstant(s,c=3):\n",
    "    # oder a constant number of c bikes \n",
    "    return min(c,M-s)\n",
    "\n",
    "def PiThreshold(s,m1=4,m2=10):\n",
    "    # if less than m1 bikes in stock, refill it up to m2\n",
    "    action = 0\n",
    "    if (s <=m1):\n",
    "        action = (m2-s)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7ffa8-1673-485d-b2a6-23e5dbc1ba0a",
   "metadata": {},
   "source": [
    "### Policy evaluation using the matrix inversion technique\n",
    "\n",
    "Since the MDP is small, if we know the reward and transition distributions we can compute the value of a policy with the matrix inversion method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694a160-5bc0-40c8-9d9f-28259f63e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StoreManagement(0,M=15,h=0.3,c=0.5,c0=0.3,p=1,q=0.1)\n",
    "s1 = 10 # initial stock \n",
    "gamma = 0.97 # discount factor \n",
    "\n",
    "# MDP parameters\n",
    "P = env.P # P[s,a,s'] = p(s' | s,a) \n",
    "r = env.r # r[s,a] = average reward received in state s when playing action a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740900a5-5f1e-4e75-965c-068044150ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    " def EvaluatePolicy(Pi):\n",
    "    # input: function (could also be vector as we can only use this method for deterministic policies)\n",
    "    r_Pi=np.zeros((M+1,1))\n",
    "    P_Pi=np.zeros((M+1,M+1))\n",
    "    for s in range(M+1):\n",
    "        P_Pi[s,:]=P[s,Pi(s),:] # matrix P^\\pi\n",
    "        r_Pi[s]=r[s,Pi(s)] # vector r^\\pi\n",
    "    V = np.linalg.inv(np.eye(M+1) - gamma * P_Pi) @ r_Pi\n",
    "    return V.transpose()[0]\n",
    "\n",
    "Values = EvaluatePolicy(PiThreshold)\n",
    "print(Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06066f60-1de9-4643-8792-7c94994c442e",
   "metadata": {},
   "source": [
    "### Policy iteration\n",
    "\n",
    "If the MDP is known, we can compute the optimal policy with policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce1843-451f-4d54-bbe1-a92e72aee397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy improvement\n",
    "def Improve(V):\n",
    "    '''computes the Q table associated to V and \n",
    "    return Pi=greedy(V)=greedy(Q) as well as max_{a} Q[s,a]'''\n",
    "    Pi = np.zeros(M+1) # improved policy \n",
    "    newV = np.zeros(M+1)\n",
    "    # compute the Q table \n",
    "    Q = np.zeros((M+1,M+1))\n",
    "    for s in range(M+1):\n",
    "        for a in range(M+1):\n",
    "            Q[s,a]=r[s,a]+gamma*np.sum([P[s,a,ns]*V[ns] for ns in range(M+1)])\n",
    "        # improvement (greedy policy wrt to Q)\n",
    "        pi = np.argmax(Q[s,:])\n",
    "        Pi[s]=pi\n",
    "        newV[s]=Q[s,pi]\n",
    "        Pi=Pi.astype(int)\n",
    "    return Pi,newV\n",
    "\n",
    "def PolicyIteration():\n",
    "    # initalization \n",
    "    Pi = np.zeros(M+1)\n",
    "    V = np.zeros(M+1)\n",
    "    # new policy \n",
    "    newPi = np.random.randint(M+1,size=M+1) \n",
    "    newPi[0]=1\n",
    "    nIt = 0 \n",
    "    while (not (Pi==newPi).all()):\n",
    "        nIt +=1 \n",
    "        Pi = np.copy(newPi)\n",
    "        # evaluate the policy (transformed into a function)\n",
    "        def PiFun(s):\n",
    "            return Pi[s]\n",
    "        V = EvaluatePolicy(PiFun)\n",
    "        newPi,x = Improve(V)\n",
    "    return Pi,V,nIt  # Policy, value, number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2f632-a840-4269-9b24-c2a21ebbbe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Pi2,V2,nIt2 = PolicyIteration()\n",
    "elapsed = time.time()-start\n",
    "\n",
    "print(\"Optimal policy is\",Pi2,\"with value \",V2,\" in \",nIt2,\" iterations and t=\",elapsed,\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148cae3-d0f3-4a84-8d52-245e65c08f18",
   "metadata": {},
   "source": [
    "# Learning in a MDP with unknown dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9138bbc-d459-4ba8-8bb2-5436047fa961",
   "metadata": {},
   "source": [
    "Now we consider reinforcement learning algorithms, who cannot make use of the knowledge of P and R, but can only simulate transistions.\n",
    "\n",
    "## Stochastic Approximation for Policy Evaluation: TD(0)\n",
    "\n",
    "TD(0) is an algorithm for policy evaluation: given a policy, we want to compute an accurate estimate of its value V.\n",
    "It can be seen as parallel Robbins-Monro algorithms, one per state.\n",
    "\n",
    "The algorithm starts from a random value vector V and a random state s0, then performs T transitions, taking actions according to the policy we want to evaluate. At each transition (from $s$ to $s'$, with reward $r$), it increments the visitation count $N(s)$ of the current state, then performs a Robbins-Monro update\n",
    "\n",
    "$V(s) \\leftarrow V(s) + \\alpha_{N(s)}(r + \\gamma V(s') - V(s))$\n",
    "\n",
    "See lecture 3 for the full pseudo-code.\n",
    "\n",
    "**Implement TD(0) with step size $\\alpha_{N(s)} = 1/\\sqrt{N(s)}$**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6c02f-f347-42a1-b055-e50bc91dae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD0(Pi, T):  # Pi: policy to be evaluated, T: number of iterations\n",
    "    V = np.random.rand(M+1) # V[s] = estimated value of each state under policy pi\n",
    "    N = np.zeros(M+1) # N[s] =number of visits to state s in the loop\n",
    "    s0 = np.random.randint(M+1)\n",
    "    env = StoreManagement(s0)\n",
    "    for t in range(T):\n",
    "        # TODO: add TD(0) loop here\n",
    "    return(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72da22c-dcf4-44e1-8095-9e582f3ee2b2",
   "metadata": {},
   "source": [
    "**Use TD(0) to compute the value of one of the simple policies defined in the introduction.**\n",
    "\n",
    "To check that value, plot the error between the estimated value of TD(0) and the value computed by matrix inversion $\\Vert V_{TD(0)} - V_{MI} \\Vert_\\infty$, as a function of the number of transitions $T$ used in TD(0).\n",
    "\n",
    "Comment on the number of iterations needed to have a good estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a389ea-d124-4a4b-a956-15c4a7ad5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089e851-624e-4484-b733-435be76928da",
   "metadata": {},
   "source": [
    "**Compare TD(0) with different updates for $\\alpha_{N(s)}$**.\n",
    "\n",
    "What exponents $\\beta$ are allowed by the theory for $\\alpha_{N(s)} = 1/N(s)^\\beta$ ?\n",
    "\n",
    "You used $\\beta = 1/2$ above. Now try $\\beta = 1$ as well as $\\beta = 0$ ($\\alpha$ constant). What works best? (look at different values of T, make plots, comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf47f6f-bc81-460d-a521-3e173b287c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3aee0-e4df-4e41-a666-bc6da8679c5e",
   "metadata": {},
   "source": [
    "## Every-visit Monte-Carlo, or TD(1)\n",
    "\n",
    "Every-visit Monte-Carlo starts from an arbitrary value vector. At each iteration, it generates a trajectory of length $H$ from a random state $s_1$. After that trajectory is done, it updates the value of each state visited $s_k$ by\n",
    "\n",
    "$V(s_k) \\leftarrow V(s_k) + \\alpha_{N(s_k)}(\\sum_{t=k}^{H}\\gamma^{t - k} r_k - V(s_k))$\n",
    "\n",
    "(implementation hint: start from the last state, and go backwards until $s_1$)\n",
    "\n",
    "See lecture 3 for more details.\n",
    "\n",
    "**Implement every-visit Monte-Carlo and compare the computed value to the value returned by policy iteration. Vary the number of trajectories and the trajectory length and comment on the effect of those parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f87250-ad6d-49fe-b080-40f4ae658439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD1(Pi, H, T):  # Pi: policy to be evaluated, H: number of iterations in a trajectory, T: number of trajectories\n",
    "    V = np.random.rand(M+1) # V[s] = estimated value of each state under policy pi\n",
    "    N = np.zeros(M+1) # N[s] =number of visits to state s in the loop\n",
    "    for t in range(T):\n",
    "        s0 = np.random.randint(M+1)\n",
    "        env = StoreManagement(s0)\n",
    "        #TODO: write your code here\n",
    "    return(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495eb402-f0a9-4f20-af3b-afc0a5148a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TD1(PiConstant, 100, 10**3)\n",
    "#TODO vary H and T."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6b974-b981-440a-ad98-339657995494",
   "metadata": {},
   "source": [
    "# Computing the optimal policy: Q-learning\n",
    "\n",
    "Q-learning does Robbins-Monro updates on all state-action pairs in order to estimate the optimal Q-value $Q^\\star$.\n",
    "Then once $Q^\\star$ is well estimated, Q-learning returns the associated greedy policy. The update after observing a transition from state $s$ and action $a$ to a state $s'$ with reward $r$ is\n",
    "\n",
    "$Q(s, a) \\leftarrow Q(s, a) + \\alpha_{N(s,a)}(r + \\gamma \\max_b Q(s', b) - Q(s, a))$\n",
    "\n",
    "See lecture 3 for a detailed pseudo-code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1ddae-8e59-4348-b8c0-1d584f260dd4",
   "metadata": {},
   "source": [
    "**Implement Q-learning**\n",
    "\n",
    "Take inspiration from your TD(0) code. For the behavior policy, use $\\varepsilon$-greedy: with probability $1 - \\varepsilon$, take the greedy(Q) action, and with probability $\\varepsilon$, take a random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0452fd0-b0c8-4548-a424-3f8e522e9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68887022-8718-4366-855e-aa5ddc41d146",
   "metadata": {},
   "source": [
    "**Compare the policy computed by Q-learning to the optimal policy (computed by policy iteration, for example)**\n",
    "\n",
    "Look at how the Q-learning policy improves with the number of transitions (with plots). Comment on the number necessary to get a policy with a good value, and on the number needed to obtain the exact optimal policy.\n",
    "\n",
    "Warning: Q-learning might need a very long time to get the exact policy (a large number of iteration, which might take very long depending on your hardware). It is fine if you don't answer the last part of the question exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2007d1a0-c81f-481c-80d1-4ae2cc83db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be20390-e44b-4066-8aa0-8f60d6b0bbad",
   "metadata": {},
   "source": [
    "**Investigate the influence of $\\varepsilon$ in the $\\varepsilon$-greedy behavior policy. Experiment with other heuristics for the behavior policy (for example softmax, and/or other ideas you may have).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723dabd-a15c-498f-a4c9-3f9c8a329f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (optional)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
