
---

# üéì Fiche de R√©vision : Introduction aux MDP (Reinforcement Learning)
**Cours :** Lecture 1 - Markov Decision Processes
**Enseignant :** R√©my Degenne (Inria/CRISTAL)

---

## 1. Concepts Fondamentaux du RL
L'Apprentissage par Renforcement (RL) est une m√©thode d'apprentissage par **"essais et erreurs"**.

*   **Objectif :** Apprendre √† agir dans un environnement stochastique (incertain) inconnu pour maximiser un signal de **r√©compense** (reward).
*   **La boucle Agent-Environnement :**
    1.  L'agent observe l'**√©tat** ($s_t$).
    2.  Il choisit une **action** ($a_t$).
    3.  L'environnement renvoie une **r√©compense** ($r_t$) et un **nouvel √©tat** ($s_{t+1}$).
*   **Diff√©rence cl√© :** Contrairement √† l'apprentissage supervis√©, il n'y a pas de "bonne r√©ponse" donn√©e, seulement un score (r√©compense) √† maximiser sur le long terme.

---

## 2. Processus de D√©cision Markovien (MDP)
Un MDP mod√©lise la prise de d√©cision s√©quentielle. Il est d√©fini par le tuple $(\mathcal{S}, \mathcal{A}, R, P)$.

### Les Composantes
*   **$\mathcal{S}$ (Espace d'√©tats) :** L'ensemble de toutes les situations possibles.
*   **$\mathcal{A}$ (Espace d'actions) :** L'ensemble des choix disponibles pour l'agent.
*   **$R$ (R√©compense) :** Distribution de la r√©compense imm√©diate.
    *   L'esp√©rance de r√©compense pour une action $a$ dans un √©tat $s$ est not√©e :
        $$r(s,a) = \mathbb{E}[R \mid s, a]$$
*   **$P$ (Transitions) :** La dynamique du syst√®me (probabilit√©s de passage d'un √©tat √† un autre).
    *   Probabilit√© d'arriver en $s'$ sachant qu'on est en $s$ et qu'on fait l'action $a$ :
        $$p(s' \mid s, a) = \mathbb{P}(s_{t+1}=s' \mid s_t=s, a_t=a)$$

### La Propri√©t√© de Markov
Le futur ne d√©pend que de l'√©tat pr√©sent, pas du pass√© (l'historique est "oubli√©").
$$\mathbb{P}(s_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, ...) = \mathbb{P}(s_{t+1} \mid s_t, a_t)$$

---

## 3. Les Politiques (Policies)
Une politique $\pi$ d√©finit le comportement de l'agent. C'est une strat√©gie pour choisir l'action en fonction de l'√©tat.

*   **Notation :** $\pi_t : \mathcal{S} \rightarrow \Delta(\mathcal{A})$ (distribution de probabilit√© sur les actions).
*   **Types de politiques :**
    *   **D√©terministe :** Une seule action choisie par √©tat ($\pi(s) = a$).
    *   **Stochastique :** Une distribution de probabilit√© sur les actions ($\pi(a|s)$).
    *   **Stationnaire :** La r√®gle ne change pas avec le temps ($\pi$ est constant).
    *   **Non-stationnaire :** La r√®gle change √† chaque pas de temps ($\pi_1, \pi_2, \dots$).

*Note : Un MDP muni d'une politique fixe devient une Cha√Æne de Markov contr√¥l√©e.*

---

## 4. Fonctions de Valeur (Value Functions)
La **Valeur** $V^\pi(s)$ estime le gain total esp√©r√© si l'on part de l'√©tat $s$ et que l'on suit la politique $\pi$.

Il existe deux formulations principales selon l'horizon de temps :

### A. Horizon Fini ($H$)
Utilis√© quand l'√©pisode a une fin d√©finie (ex: une partie de jeu).
$$V^\pi(s) = \mathbb{E}^\pi \left[ \sum_{t=1}^{H} r_t \mid s_1 = s \right]$$

### B. Horizon Infini avec facteur d'escompte ($\gamma$)
Utilis√© pour les processus continus. Le param√®tre $\gamma \in (0, 1)$ (discount factor) donne moins d'importance aux r√©compenses lointaines pour assurer la convergence.
$$V^\pi(s) = \mathbb{E}^\pi \left[ \sum_{t=1}^{\infty} \gamma^{t-1} r_t \mid s_1 = s \right]$$

---

## 5. Optimalit√© et √âquations de Bellman

### Politique Optimale
L'objectif est de trouver une politique $\pi^*$ qui maximise la valeur dans chaque √©tat :
$$V^*(s) = \max_{\pi} V^\pi(s)$$
*Th√©or√®me :* Il existe toujours une politique optimale qui est **d√©terministe**.

### √âquation de Bellman (Cas Horizon Fini)
La valeur d'un √©tat d√©pend de la r√©compense imm√©diate et de la valeur de l'√©tat suivant.
Pour une politique d√©terministe $\pi$ √† l'√©tape $h$ :

$$V^\pi_h(s) = r(s, \pi_h(s)) + \sum_{s' \in \mathcal{S}} p(s' \mid s, \pi_h(s)) V^\pi_{h+1}(s')$$

*(Avec la convention que la valeur √† la fin de l'horizon $V_{H+1} = 0$).*

---

## 6. R√©sum√© du probl√®me RL global
Dans un MDP **connu** (on connait $r$ et $p$), on peut calculer la politique optimale par programmation dynamique (planification).

Le v√©ritable **Reinforcement Learning** consiste √† trouver cette politique optimale dans un MDP **inconnu**, uniquement en interagissant avec l'environnement (en observant les transitions et les r√©compenses r√©elles).