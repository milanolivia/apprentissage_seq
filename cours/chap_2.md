
---

# üéì Fiche de R√©vision : Programmation Dynamique
**Cours :** Lecture 2 - Dynamic Programming
**Enseignant :** R√©my Degenne (Inria/CRISTAL)

---

## 1. Contexte et Objectifs
Dans ce cours, on suppose que le MDP est **connu** (on conna√Æt les mod√®les de transition $P$ et de r√©compense $R$). L'objectif est de **calculer** (et non pas encore d'apprendre) la politique optimale $\pi^*$ et sa valeur $V^*$.

L'ensemble des m√©thodes pour r√©soudre ces probl√®mes d'optimisation s√©quentielle s'appelle la **Programmation Dynamique**.

---

## 2. Cas 1 : Horizon Fini ($H$)
Le processus s'arr√™te apr√®s un nombre de pas $H$ connu. La politique optimale peut √™tre non-stationnaire (d√©pend du temps $t$).

### M√©thode : Induction Arri√®re (Backwards Induction)
On commence par la fin ($t=H$) et on remonte jusqu'au d√©but.

**1. √âvaluation d'une politique $\pi$ :**
Pour calculer $V^\pi_h(s)$, on utilise l'√©quation de Bellman r√©cursive :
$$V_{h}^{\pi}(s) = r(s,\pi_h(s)) + \sum_{s'} p(s'|s,\pi_h(s)) V_{h+1}^{\pi}(s')$$
*(Convention : $V_{H+1}(s) = 0$)*.

**2. Calcul de la politique optimale :**
On calcule $V^*$ en prenant le max sur les actions √† chaque √©tape :
$$V_{h}^{*}(s) = \max_{a \in \mathcal{A}} \left[ r(s,a) + \sum_{s'} p(s'|s,a) V_{h+1}^{*}(s') \right]$$

*   **Complexit√© temporelle :** $O(S^2 A H)$
*   **Complexit√© spatiale :** $O(S H)$

---

## 3. Cas 2 : Horizon Infini avec Escompte ($\gamma$)
Le processus ne s'arr√™te pas, mais les r√©compenses futures sont att√©nu√©es par $\gamma \in (0,1)$. On cherche une politique **stationnaire** (ne d√©pend pas de $t$).

### A. √âvaluation de Politique (Policy Evaluation)
Pour une politique $\pi$ fix√©e, $V^\pi$ est solution de l'√©quation lin√©aire de Bellman :
$$V^{\pi}(s) = r(s,\pi(s)) + \gamma \sum_{s'} p(s'|s,\pi(s)) V^{\pi}(s')$$

Sous forme matricielle : $V^\pi = r^\pi + \gamma P^\pi V^\pi$.
Deux m√©thodes de r√©solution :
1.  **Directe (Inversion matricielle) :** $V^\pi = (I - \gamma P^\pi)^{-1} r^\pi$. (Co√ªteux si $S$ est grand : $O(S^3)$).
2.  **It√©rative :** On applique l'op√©rateur de Bellman $T^\pi$ de fa√ßon r√©p√©t√©e : $V_{k+1} = T^\pi(V_k)$.

### B. L'Op√©rateur de Bellman ($T^\pi$ et $T^*$)
C'est le concept cl√© qui garantit la convergence des algorithmes.

*   **Op√©rateur $T^\pi$ (pour une politique donn√©e) :**
    $$T^{\pi}(V)(s) = r(s,\pi(s)) + \gamma \sum_{s'} p(s'|s,\pi(s)) V(s')$$
*   **Op√©rateur Optimal $T^*$ (maximisation) :**
    $$T^{*}(V)(s) = \max_{a} \left[ r(s,a) + \gamma \sum_{s'} p(s'|s,a) V(s') \right]$$

**Propri√©t√© fondamentale (Th√©or√®me du point fixe de Banach) :**
Ces op√©rateurs sont des **$\gamma$-contractions**. Cela signifie que :
1.  Il existe une **unique** solution (point fixe) $V^*$ telle que $T^*(V^*) = V^*$.
2.  Appliquer l'op√©rateur de fa√ßon r√©p√©t√©e converge toujours vers cette solution, peu importe le point de d√©part $V_0$.

---

## 4. Algorithmes de R√©solution (Horizon Infini)

Il existe deux algorithmes principaux pour trouver la politique optimale.

### Algorithme 1 : Value Iteration (VI)
On cherche directement $V^*$ en it√©rant l'op√©rateur optimal $T^*$, puis on en d√©duit la politique.

1.  **Initialiser** $V_0$ arbitrairement.
2.  **R√©p√©ter** jusqu'√† convergence ($||V_{k+1} - V_k|| < \epsilon$) :
    $$V_{k+1}(s) \leftarrow \max_{a} \left( r(s,a) + \gamma \sum_{s'} p(s'|s,a) V_k(s') \right)$$
3.  **Retourner** la politique "greedy" (gloutonne) par rapport √† $V_{final}$.

### Algorithme 2 : Policy Iteration (PI)
On alterne entre √©valuer la politique actuelle et l'am√©liorer.

1.  **Initialiser** une politique $\pi_0$.
2.  **R√©p√©ter** jusqu'√† ce que la politique ne change plus ($\pi_{k+1} = \pi_k$) :
    *   **√âvaluation :** Calculer $V^{\pi_k}$ (par inversion de matrice ou it√©rations).
    *   **Am√©lioration :** $\pi_{k+1} \leftarrow \text{greedy}(V^{\pi_k})$.
    $$\pi_{k+1}(s) = \arg\max_{a} \left( r(s,a) + \gamma \sum_{s'} p(s'|s,a) V^{\pi_k}(s') \right)$$
3.  **Retourner** $\pi^*$.

---

## 5. Comparaison VI vs PI

| Caract√©ristique | Value Iteration (VI) | Policy Iteration (PI) |
| :--- | :--- | :--- |
| **Principe** | Met √† jour les valeurs ($V$) directement. | Alterne √©valuation compl√®te ($V^\pi$) et mise √† jour de $\pi$. |
| **Complexit√© par it√©ration** | Plus faible ($O(S^2 A)$). | Plus √©lev√©e (n√©cessite une √©valuation compl√®te, souvent $O(S^3)$). |
| **Nombre d'it√©rations** | √âlev√© (convergence asymptotique). | Faible (converge souvent tr√®s vite en nombre d'√©tapes). |
| **Garantie** | Approche $V^*$ √† $\epsilon$ pr√®s. | Trouve la politique optimale exacte $\pi^*$ en temps fini. |

---

## 6. Impl√©mentation via Q-Values
Pour √©viter de recalculer les sommes $\sum p(s'|s,a)$ lors de l'√©tape "greedy" ou de l'extraction de la politique, on stocke souvent les **Q-Values** :

$$Q(s,a) = r(s,a) + \gamma \sum_{s'} p(s'|s,a) V(s')$$

*   Relation : $V(s) = \max_a Q(s,a)$.
*   Politique gloutonne : $\pi(s) = \arg\max_a Q(s,a)$.
*   Complexit√© spatiale : $O(S \times A)$.