
---

# üìö Fiche de R√©vision : Algorithmes d'Apprentissage par Renforcement (Lecture 3)

Cette fiche synth√©tise le passage de la Programmation Dynamique (mod√®le connu) aux m√©thodes d'Apprentissage par Renforcement (mod√®le inconnu) telles que Monte Carlo, TD Learning, Q-Learning et SARSA.

---

## 1. Introduction & Contexte

En **Apprentissage par Renforcement (RL)**, contrairement √† la Programmation Dynamique, l'agent ne conna√Æt pas les param√®tres du MDP (les r√©compenses $r$ et les probabilit√©s de transition $p$).

* **Protocole d'interaction :** L'agent apprend en interagissant avec l'environnement et en observant des transitions $(s_t, a_t, r_t, s_{t+1})$.
* **Objectif :** Estimer la fonction de valeur optimale $V^*$ ou trouver la politique optimale $\pi^*$ √† partir de l'historique de ces observations.

---

## 2. Fondements Math√©matiques : Approximation Stochastique

Pour apprendre sans mod√®le, on utilise des m√©thodes d'estimation it√©ratives bas√©es sur des √©chantillons.

### A. Estimation Monte-Carlo (MC)
Une m√©thode na√Øve pour estimer la valeur $V^\pi(s)$ est de calculer la moyenne des retours cumul√©s sur plusieurs trajectoires compl√®tes partant de l'√©tat $s$.

* **Formule it√©rative :** On peut mettre √† jour la moyenne progressivement :
    $$\hat{\mu}_n = \hat{\mu}_{n-1} + \frac{1}{n}(Z_n - \hat{\mu}_{n-1})$$
* **Limite :** Il faut attendre la fin de l'√©pisode (trajectoire compl√®te) pour effectuer une mise √† jour.

### B. Algorithme de Robbins-Monro
C'est la base th√©orique de la convergence des algorithmes de RL. Il s'agit de trouver un point fixe avec des √©valuations bruit√©es via la mise √† jour :
$$x_n = x_{n-1} + \alpha_n Y_n$$

* **Conditions de convergence :** Pour garantir la convergence, les pas d'apprentissage (learning rates) $\alpha_n$ doivent satisfaire :
    1.  $\sum \alpha_n = \infty$ (suffisant pour atteindre la cible).
    2.  $\sum \alpha_n^2 < \infty$ (d√©croissant pour stabiliser la variance).

---

## 3. Temporal Difference (TD) Learning

Le TD Learning combine l'√©chantillonnage (comme Monte Carlo) et le *bootstrapping* (comme la Programmation Dynamique).

### A. TD(0) : √âvaluation de Politique
L'agent met √† jour l'estimation $V(s)$ imm√©diatement apr√®s chaque transition $(s, r, s')$, sans attendre la fin de l'√©pisode.

* **L'erreur TD :** $\delta_k = r_k + \gamma \hat{V}(s_{k+1}) - \hat{V}(s_k)$.
* **R√®gle de mise √† jour :**
    $$V(s) \leftarrow V(s) + \alpha (r + \gamma V(s') - V(s))$$
* **Bootstrapping :** L'estimation est mise √† jour en se basant sur une autre estimation ("La valeur de l'estimation est d√©plac√©e vers la valeur de la nouvelle estimation").
* **Convergence :** Converge vers $V^\pi$ si les pas respectent les conditions de Robbins-Monro.

### B. TD(1) / Every-visit Monte-Carlo
C'est une extension o√π l'on met √† jour les valeurs de tous les √©tats visit√©s dans une trajectoire √† la fin de celle-ci.

---

## 4. Q-Learning : Trouver la Politique Optimale

L'objectif est d'apprendre directement la valeur $Q^*$ (action-√©tat) optimale pour en d√©duire la politique optimale, sans conna√Ætre le mod√®le.

### A. Algorithme
Q-Learning est un algorithme **Off-policy** (hors politique).

* **Principe :** Il utilise l'√©quation de Bellman optimale pour la mise √† jour.
* **R√®gle de mise √† jour :**
    $$Q(s,a) \leftarrow Q(s,a) + \alpha \left( r + \gamma \max_{b \in \mathcal{A}} Q(s', b) - Q(s,a) \right)$$
* **Convergence :** Converge vers $Q^*$ si toutes les paires $(s,a)$ sont visit√©es infiniment souvent et si les pas $\alpha$ sont appropri√©s.

### B. Exploration
L'exploration est cruciale pour visiter tous les √©tats.
* **$\epsilon$-greedy :** Choisit une action al√©atoire avec probabilit√© $\epsilon$, sinon choisit la meilleure action connue.
* **Boltzmann (Softmax) :** Choisit les actions selon une probabilit√© proportionnelle √† leur valeur estim√©e (temp√©rature $\tau$).

---

## 5. Actor/Critic et SARSA

L'architecture Actor/Critic distingue l'entit√© qui agit (Actor) de celle qui √©value l'action (Critic).

### SARSA (State-Action-Reward-State-Action)
C'est une m√©thode **On-policy** (sur politique). Le critique √©value la politique *actuellement suivie* par l'acteur.

* **R√®gle de mise √† jour :**
    $$\hat{Q}(s_t,a_t) \leftarrow \hat{Q}(s_t,a_t) + \alpha \left( r_t + \gamma \hat{Q}(s_{t+1}, a_{t+1}) - \hat{Q}(s_t,a_t) \right)$$
* **Diff√©rence avec Q-Learning :** SARSA utilise l'action $a_{t+1}$ r√©ellement effectu√©e (qui peut √™tre une action d'exploration), alors que Q-Learning utilise le $\max$ sur toutes les actions possibles.

---

## 6. Comparaison : Q-Learning vs SARSA

L'exemple du "Cliff" (la falaise) illustre la diff√©rence de comportement.

| Caract√©ristique | Q-Learning | SARSA |
| :--- | :--- | :--- |
| **Type** | **Off-policy** | **On-policy** |
| **Cible** | Apprend la valeur de la politique optimale th√©orique ($\max$). | Apprend la valeur de la politique de comportement (exploratoire). |
| **Comportement** | Emprunte le chemin optimal (au bord de la falaise) mais tombe souvent pendant l'apprentissage. | Emprunte un chemin plus s√ªr (loin du bord) pour √©viter les chutes dues √† l'exploration al√©atoire. |
| **Convergence** | Converge vers $Q^*$ (optimal). | Converge vers une politique proche de l'optimale si l'exploration ($\epsilon$) diminue. |

---
