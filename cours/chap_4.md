Voici une fiche de r√©vision structur√©e en Markdown pour le cours "Reinforcement Learning with Function Approximation" (Lecture 4), bas√©e sur le document fourni.

---

# üìö Fiche de R√©vision : RL avec Approximation de Fonction (Lecture 4)

[cite_start]Cette fiche aborde le passage des m√©thodes tabulaires (espaces d'√©tats finis et petits) aux m√©thodes utilisant l'approximation de fonction pour g√©rer de grands espaces d'√©tats ou des dynamiques inconnues [cite: 16-18].

## 1. Contexte et Motivation

* [cite_start]**Probl√®me des m√©thodes tabulaires :** Pour de grands espaces d'√©tats (ex : pixels d'un jeu vid√©o, robotique, conduite autonome), il est impossible de stocker une valeur pour chaque √©tat $V(s)$ ou chaque paire √©tat-action $Q(s,a)$ en m√©moire [cite: 108-114, 200].
* [cite_start]**Solution :** Utiliser une **approximation param√©trique** $V_\theta(s)$ ou $Q_\theta(s,a)$ o√π $\theta$ est un vecteur de param√®tres de taille r√©duite (ex : poids d'un r√©seau de neurones) [cite: 201-205].


---

## 2. Choix de la cible : V ou Q ?

* [cite_start]**Apprendre $V^*$ (Value Function) :** Si la dynamique $p(s'|s,a)$ est connue, on peut d√©duire la politique optimale via $\pi(s) = \text{argmax}_a (r + \gamma \mathbb{E}[V(s')])$[cite: 139].
* [cite_start]**Apprendre $Q^*$ (Action-Value Function) :** Si la dynamique est inconnue (cas standard du RL), il est n√©cessaire d'apprendre $Q$ pour agir, car la politique greedy est $\pi(s) = \text{argmax}_a Q(s,a)$ [cite: 172-173].
* [cite_start]**Impact de l'erreur :** Si $V$ est une approximation de $V^*$, la perte de performance de la politique induite est born√©e par l'erreur d'approximation $||V^* - V||_\infty$[cite: 182].

---

## 3. Types d'Approximation

### A. Approximation Lin√©aire
La valeur est une combinaison lin√©aire de **features** (caract√©ristiques) $\phi(s)$ :
$$V_\theta(s) = \theta^\top \phi(s) = \sum_{i=1}^d \theta_i \phi_i(s)$$
* [cite_start]**Avantage :** Le gradient est simple : $\nabla_\theta V_\theta(s) = \phi(s)$ [cite: 236-241].
* [cite_start]**Exemples de features :** Polyn√¥mes, bases de Fourier, RBF (Radial Basis Functions), ou tuiles (Tile coding) [cite: 256-260, 216].

### B. Approximation Non-lin√©aire
Utilisation de **R√©seaux de Neurones** (Deep RL).
* [cite_start]**Avantage :** "Apprennent" les features directement depuis les donn√©es brutes (ex: pixels) et sont des approximateurs universels [cite: 267-268].


---

## 4. √âvaluation de Politique (Policy Evaluation)

[cite_start]L'objectif est de trouver $\theta$ qui minimise l'erreur quadratique moyenne (MSVE - Mean Square Value Error) par rapport √† la vraie valeur $V^\pi$[cite: 297].

### A. Descente de Gradient Stochastique (SGD)
On voudrait faire une mise √† jour type gradient :
$$\theta \leftarrow \theta + \alpha (V^\pi(s_t) - V_\theta(s_t)) \nabla_\theta V_\theta(s_t)$$
* [cite_start]**Probl√®me :** On ne conna√Æt pas la cible $V^\pi(s_t)$[cite: 356].

### B. Semi-Gradient TD(0)
[cite_start]On remplace la cible inconnue $V^\pi(s_t)$ par l'estimation bootstrap (TD target) : $r_t + \gamma V_{\theta}(s_{t+1})$[cite: 367].
* **Mise √† jour :**
    [cite_start]$$\theta_{t} \leftarrow \theta_{t-1} + \alpha_{t}(r_{t} + \gamma V_{\theta_{t-1}}(s_{t+1}) - V_{\theta_{t-1}}(s_{t})) \nabla_{\theta}V_{\theta_{t-1}}(s_{t})$$[cite: 367].
* [cite_start]**Convergence :** Dans le cas lin√©aire, converge vers un point fixe $\theta_{TD}$ qui est la solution projet√©e du point fixe de Bellman [cite: 392-393].

### C. LSTD (Least Square Temporal Difference)
[cite_start]Dans le cas lin√©aire, au lieu de faire du gradient pas √† pas, on peut r√©soudre directement le syst√®me lin√©aire $A \theta = b$ pour trouver l'optimum [cite: 453-455].
* [cite_start]**Efficacit√© :** Converge plus vite que TD(0) (meilleure utilisation des donn√©es) mais co√ªte plus cher en calcul ($O(d^2)$ pour inverser la matrice)[cite: 480].

---

## 5. Contr√¥le (Trouver la politique optimale)

Comment adapter *Policy Iteration* ou *Value Iteration* avec de l'approximation ?

### A. Approximate Dynamic Programming
1.  [cite_start]**LSPI (Least-Squares Policy Iteration) :** Utilise une version LSTD pour estimer $Q^\pi$ (LSTD-Q) √† l'int√©rieur d'une boucle Policy Iteration[cite: 545].
2.  **Fitted-Q Iteration (FQI) :** Une approche offline bas√©e sur *Value Iteration*.
    * [cite_start]√Ä chaque it√©ration $k$, on construit un dataset d'entra√Ænement avec pour cibles $y_i = r_i + \gamma \max_a Q_{k-1}(s'_i, a)$[cite: 597].
    * [cite_start]On entra√Æne un r√©gresseur (ex: moindres carr√©s, arbres de d√©cision) pour pr√©dire $y_i$ √† partir de $(s_i, a_i)$[cite: 580, 599].

### B. Approximate Q-Learning
Extension directe de Q-Learning avec descente de gradient (semi-gradient).
* **Mise √† jour :**
    [cite_start]$$\theta \leftarrow \theta + \alpha (r + \gamma \max_{b} Q_\theta(s', b) - Q_\theta(s, a)) \nabla_\theta Q_\theta(s, a)$$[cite: 710].
* [cite_start]**Risque :** Connu pour diverger si on utilise des approximations non-lin√©aires (r√©seaux de neurones)[cite: 716].

---

## 6. Deep Q Networks (DQN)

[cite_start]Pour stabiliser l'apprentissage de Q-Learning avec des r√©seaux de neurones, DQN introduit plusieurs astuces[cite: 734]:

1.  [cite_start]**Experience Replay (Replay Buffer) :** Stocker les transitions $(s, a, r, s')$ pass√©es dans un tampon et apprendre sur des mini-batchs al√©atoires pour casser les corr√©lations temporelles[cite: 735, 746].
2.  [cite_start]**Target Network :** Utiliser un r√©seau cible param√©tr√© par $\theta^-$ (copie fig√©e du r√©seau principal) pour calculer la cible $r + \gamma \max Q_{\theta^-}(s', b)$, mis √† jour moins souvent[cite: 747, 792].
3.  [cite_start]**R√©sultats :** A permis d'atteindre des performances surhumaines sur les jeux Atari[cite: 800].