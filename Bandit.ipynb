{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff2be9f-1ebb-4c18-9ffc-cf21bf42ccc3",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- Follow the installation instructions in the readme file\n",
    "- Answer the questions in this notebook\n",
    "- Once your work is finished: restart the kernel, run all cells in order and check that the outputs are correct.\n",
    "- Send your completed notebook to `remy.degenne@inria.fr` with email title `SL_TP3_NAME1_NAME2` (or `SL_TP3_NAME` if you work alone).\n",
    "\n",
    "**Deadline: January 30, 11:59 AM CET**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3bc37-a626-4c3e-9247-67cb5d69b6c3",
   "metadata": {},
   "source": [
    "If you don't want to use a local installation, you can try Google Colab:\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/RemyDegenne/remydegenne.github.io/blob/master/docs/SL_2025/Bandit.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb22d5-829c-4ab9-87c0-bda6632c8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is setting up google colab. Ignore it if you work locally.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"Installing packages, please wait a few moments. Restart the runtime after the installation.\")\n",
    "    # install rlberry library\n",
    "    !pip install scipy rlberry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577dc7b-55a2-49aa-a0e1-80a989d0ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.stats as st\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "rng = np.random.default_rng(45161641614384786754156467846542343184353443258463569)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88254da1-16d6-4731-a42d-a9460deee054",
   "metadata": {},
   "source": [
    "# Bandits\n",
    "\n",
    "Bandits can be seen as RL with only one state. A bandit environment with $K$ actions (or arms) is parametrized by $K$ distributions $\\nu_1, \\ldots, \\nu_K$ with finite means $\\mu_1, \\ldots, \\mu_K$. At each time $t \\in \\mathbb{N}$, an agent\n",
    "- chooses an arm $A_t \\in \\{1, \\ldots, K\\}$ (possibly based on previous observations)\n",
    "- gets a reward $X_{A_t, t} \\sim \\nu_{A_t}$ samples from the distribution of arm $A_t$\n",
    "\n",
    "In the regret minimization setting, the agent seeks to minimize its expected regret at some horizon $T$,\n",
    "$$R_T = T \\max_k \\mu_k - \\sum_{t = 1}^T \\mu_{A_t} \\: .$$\n",
    "\n",
    "More precisely, we will evaluate agents/algorithms based on their expected regret $\\mathbb{E}[R_T]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad59c0-1ff6-424f-9bce-e77018ef1dcb",
   "metadata": {},
   "source": [
    "**Implement a function that computes the regret of a sequence of actions**\n",
    "\n",
    "The result will be an array of length T = len(actions) containing the values of $R_t$ for all $t < T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bab7f0-3f95-41bd-88fe-660f1aa9aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret(means, actions):\n",
    "    \"\"\"\n",
    "    Compute the regret of a sequence of actions on a bandit problem with given means\n",
    "\n",
    "    means: numpy.ndarray; vector of means of the arm distributions, of size K\n",
    "    actions: list of int in 0, ..., K-1; sequence of actions\n",
    "\n",
    "    Returns: numpy.ndarray; regret of the sequence of actions at each time t <= len(actions)\n",
    "    \"\"\"\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c8829a-2944-4514-8a3a-a9715ea67502",
   "metadata": {},
   "source": [
    "### Bandit environments\n",
    "\n",
    "We will test our bandits algorithms on Gaussian distributions with variance 1 and on Bernoulli distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d747007-8c3b-4d39-829a-e8e9685fc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self, distrib):\n",
    "        self.distributions = distrib\n",
    "        self.means = [dist.mean() for dist in distrib]\n",
    "\n",
    "    def sample(self, arm, n=1):\n",
    "        # return n samples from one arm distribution, as a numpy array\n",
    "        return self.distributions[arm].rvs(n)\n",
    "\n",
    "class GaussianBandit(Bandit):\n",
    "    def __init__(self, means):\n",
    "        distrib = [st.norm(loc=m) for m in means]\n",
    "        super().__init__(distrib) \n",
    "\n",
    "class BernoulliBandit(Bandit):\n",
    "    def __init__(self, means):\n",
    "        distrib = [st.bernoulli(p=m) for m in means]\n",
    "        super().__init__(distrib) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e4075-8093-44f6-9800-677ca6cf3348",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee84be-bc6a-4986-8ed1-34b49fbb489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([0.3, 0.5, 0.25])\n",
    "K = len(mu)\n",
    "B1 = GaussianBandit(mu)\n",
    "B2 = BernoulliBandit(mu)\n",
    "print(B1.sample(2, 10))  # 10 samples of arm 2 in bandit B1\n",
    "print(B1.means[2])  # mean of arm 2 in bandit B1\n",
    "print(B2.sample(2))  # 1 sample of arm 2 in bandit B2\n",
    "print(B2.means[2])  # mean of arm 2 in bandit B2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e1268-8567-4608-95a1-4b9ecb89d717",
   "metadata": {},
   "source": [
    "# Failure of Follow-The-Leader\n",
    "\n",
    "The Follow-The-Leader algorithm (FTL) for bandits pulls each arm once and then pulls at each time the arm with highest empirical mean. That is, it pulls\n",
    "$$\n",
    "A_t = \\arg\\max_a \\hat{\\mu}_{t,a}\n",
    "$$\n",
    "where $\\hat{\\mu}_{t,a} = \\frac{1}{N_{t,a}}\\sum_{s=1}^{t-1} X_{A_s, s} \\mathbb{I}\\{A_s = a\\}$ and $N_{t,a} = \\sum_{s=1}^{t-1} \\mathbb{I}\\{A_s = a\\}$.\n",
    "\n",
    "**What is in general the dependence in the horizon $T$ of the expected regret of FTL?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d486e-5b29-47d3-a418-d68c22707439",
   "metadata": {},
   "source": [
    "#TODO answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e530a23-182d-4864-aa06-5660c67f0cdb",
   "metadata": {},
   "source": [
    "**Implement the Follow-The-Leader algorithm**\n",
    "\n",
    "In general, a bandit algorithm uses the whole sequence of past interactions to decide which arm should be pulled next. However, all algorithm we will implement in this notebook are functions of the number of past pulls of the arms and their empirical means, as well as the current time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d30b0-c98e-485c-bb43-eb2b60e96c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FTL(emp_means, nb_pulls, t):\n",
    "    \"\"\"\n",
    "    Follow-The-Leader arm selection\n",
    "\n",
    "    emp_means: numpy.ndarray; vector of empirical means of the arms\n",
    "    nb_pulls: numpy.ndarray; number of times each arm was chosen previously\n",
    "    t: int; current time\n",
    "    \n",
    "    Returns: int; arm selected by FTL\n",
    "    \"\"\"\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcee33-97fb-4a47-a38a-c3cdb91d7703",
   "metadata": {},
   "source": [
    "**Plot the mean regret of FTL on a Gaussian bandit with 4 arms with means [0.3, 0.5, 0.25, 0.1] as a function of $t$ up to $T = 2000$, averaged over 30 experiments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d192009-f25b-40e6-a98d-78777310c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "N = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe7821-92a1-43ed-ae96-126005a831c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64a24a-e872-45a5-a353-63e002f2e4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b705b41",
   "metadata": {},
   "source": [
    "**Look at several individual runs of FTL (in particular look at the sequence of actions). How can we explain the bad expected regret of FTL?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44e01e",
   "metadata": {},
   "source": [
    "#TODO answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55a82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d739145",
   "metadata": {},
   "source": [
    "# $\\varepsilon$-greedy\n",
    "\n",
    "In order to obtain sub-linear regret, an algorithm should balance *exploration* and *exploitation*. The $\\varepsilon$-greedy algorithm selects with probability $\\varepsilon$ an arm uniformly at random (exploration) and with probability $1 - \\varepsilon$ it chooses the empirical best arm like FTL.\n",
    "\n",
    "**Implement $\\varepsilon$-greedy for a decreasing $\\varepsilon_t = c/t$, where $c$ is a parameter and $t$ is the current time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26510570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(emp_means, nb_pulls, t):\n",
    "    \"\"\"\n",
    "    eps-greedy arm selection\n",
    "\n",
    "    emp_means: numpy.ndarray; vector of empirical means of the arms\n",
    "    nb_pulls: numpy.ndarray; number of times each arm was chosen previously\n",
    "    t: int; current time\n",
    "    \n",
    "    Returns: int; arm selected by eps-greedy\n",
    "    \"\"\"\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae13cc-4562-4708-86eb-33cbc7381378",
   "metadata": {},
   "source": [
    "# UCB\n",
    "\n",
    "The UCB algorithm is optimistic: it selects the arm with highest upper confidence bound. Using concentration of measure arguments, we can construct confidence intervals on the means of each arm, such that the true mean belongs to the interval with large enough probability. UCB then selects the arm with highest upper bound for the confidence interval.\n",
    "\n",
    "The algorithm starts by pulling all arms: $A_t = t$ for $t \\le K$. Then, using a confidence interval obtained from a 1-sub-Gaussian assumption, UCB selects\n",
    "\n",
    "$$A_t = \\arg\\max_a \\hat{\\mu}_{t,a} + \\sqrt{\\frac{2 \\log t}{N_{t,a}}} \\: .$$\n",
    "\n",
    "\n",
    "\n",
    "**Implement the UCB algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2426c706-dcfc-4f90-b610-c192fc93beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB(emp_means, nb_pulls, t):\n",
    "    \"\"\"\n",
    "    UCB arm selection\n",
    "\n",
    "    emp_means: numpy.ndarray; vector of empirical means of the arms\n",
    "    nb_pulls: numpy.ndarray; number of times each arm was chosen previously\n",
    "    t: int; current time\n",
    "    \n",
    "    Returns: int; arm selected by UCB\n",
    "    \"\"\"\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ebd3f-34e8-4b44-b21a-c9a636aebd4f",
   "metadata": {},
   "source": [
    "**Plot the mean regret of FTL, $\\varepsilon_t$-greedy and UCB on a Gaussian bandit with 4 arms with means [0.3, 0.5, 0.25, 0.1] as a function of $t$ up to $T = 2000$, averaged over 30 experiments.**\n",
    "\n",
    "See the lecture notes for a good theoretical choice of the parameter $c$ in $\\varepsilon_t$-greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f42cd-7666-4640-8cc0-aa1e4d547088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef77937-b616-4768-95f1-082f45f6550a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2e3060-5847-4196-9179-b278b727a94f",
   "metadata": {},
   "source": [
    "**Comment on the graph. What is the dependence in $T$ of the regret of UCB?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf099e29-ba81-46db-a840-d21523710feb",
   "metadata": {},
   "source": [
    "#TODO answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0cd8c-9eb9-4e02-882b-a4e8fd7c3eb1",
   "metadata": {},
   "source": [
    "Consider the following variant UCB(c), which takes a parameter $c>0$,\n",
    "$$A_t = \\arg\\max_a \\hat{\\mu}_{t,a} + \\sqrt{\\frac{c \\log t}{N_{t,a}}} \\: .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aaae21-85cc-4d37-adb0-c0b168e61d5d",
   "metadata": {},
   "source": [
    "**Implement UCB(c) and plot the regret of UCB(c) for varying values of $c$, first for the Gaussian bandit, then for a Bernoulli bandit with same means. Discuss.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb499430-eb1d-4124-8284-60376507a0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3de87bcd-dc37-4a8a-8e2d-b36dc455c028",
   "metadata": {},
   "source": [
    "# Thompson Sampling\n",
    "\n",
    "The Thompson Sampling (TS) algorithm takes a Bayesian approach (even though the regret metric is a frequentist way of evaluating an algorithm).\n",
    "\n",
    "The algorithm postulates a prior distribution for the means of the arms, say a Gaussian $\\mathcal{N}(0,1)$. Then after each pull, it updates the posterior distribution of the mean of each arm.\n",
    "\n",
    "After $N_{t,a}$ observations sampled from a Gaussian distribution with variance 1, with average reward $\\hat{\\mu}_{t,a}$, arm $a$ has posterior distribution $P_{a,t} = \\mathcal N(\\hat{\\mu}_{t,a} \\frac{N_{t,a}}{N_{t,a} + 1}, \\frac{1}{N_{t,a} + 1})$.\n",
    "\n",
    "At time $t$, the algorithm then samples the posterior distribution of each arm, and pulls the arm with highest sample. That is, it samples $\\theta_{a,t} \\sim P_{a,t}$ for all $a \\in [K]$, then samples $A_t = \\arg\\max_a \\theta_{a,t}$.\n",
    "\n",
    "**Implement Gaussian Thompson Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063727c5-d978-4a78-b71d-5f4ddd74c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TS(emp_means, nb_pulls, t):\n",
    "    \"\"\"\n",
    "    TS arm selection\n",
    "\n",
    "    emp_means: numpy.ndarray; vector of empirical means of the arms\n",
    "    nb_pulls: numpy.ndarray; number of times each arm was chosen previously\n",
    "    t: int; current time\n",
    "    \n",
    "    Returns: int; arm selected by TS\n",
    "    \"\"\"\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e7e84-e1fd-4313-94ef-00a1932051c9",
   "metadata": {},
   "source": [
    "**Plot the regret of Gaussian TS and UCB on the Gaussian bandit from previous questions, as well as on a Bernoulli bandit with same means. Discuss.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d735815-ea8e-46b1-9e56-5df333dfd250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e5b02-ea46-4f08-9f24-8d470232719c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce17face-b9b1-4620-9382-59ee250bc55c",
   "metadata": {},
   "source": [
    "Thompson sampling can be adapted to the reward distributions by changing the prior: if the rewards are Gaussian, a Gaussian prior makes sense and leads to a Gaussian posterior. If the rewards are Bernoulli, we can use a uniform prior, which leads to a Beta posterior (see lecture 6 for the precise parameters of the Beta posterior).\n",
    "\n",
    "**Implement TS with uniform prior and Beta posterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ddbcf-738e-43a3-a02d-b0a708c8ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TS_beta(emp_means, nb_pulls, t):\n",
    "    \"\"\"\n",
    "    TS arm selection\n",
    "\n",
    "    emp_means: numpy.ndarray; vector of empirical means of the arms\n",
    "    nb_pulls: numpy.ndarray; number of times each arm was chosen previously\n",
    "    t: int; current time\n",
    "    \n",
    "    Returns: int; arm selected by TS\n",
    "    \"\"\"\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c8084-20f6-45f5-b8aa-71cb92a44c4e",
   "metadata": {},
   "source": [
    "**On a Gaussian bandit, compare the regret of TS with Gaussian posterior, TS with Beta posterior, and UCB.**\n",
    "\n",
    "(note that even though the Beta is a true posterior for the uniform prior only for Benoulli rewards, we can still run the algorithm on Gaussian rewards while pretending that the Beta is a posterior, and see what happens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86770aaf-7778-46c1-95a4-5e715afa1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaaf9b8-6f02-4143-921b-f85d9ca12293",
   "metadata": {},
   "source": [
    "**On a Bernoulli bandit, compare the regret of TS with Gaussian prior, TS with Beta posterior, and UCB.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3575c38-8171-4f3f-aeb1-ffa53262f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a73d32-58b4-4de1-899f-a03c32a97c95",
   "metadata": {},
   "source": [
    "**How could the UCB algorithm be improved to have a lower regret on Bernoulli bandits, if we know beforehand that the distributions are Bernoulli?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6accd036-8ca3-4b8a-8de1-ea65b96d80a1",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb01aaf-7eab-4b6b-87a1-c8dbde3a66f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
